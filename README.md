# Sequence2Sequence-Implementation

The Seq2Seq framework relies on the encoder-decoder paradigm. The encoder encodes the input sequence, while the decoder produces the target sequence

### Encoder

Our input sequence is how are you. Each word from the input sequence is associated to a vector  ğ‘¤âˆˆâ„ğ‘‘  (via a lookup table). In our case, we have 3 words, thus our input will be transformed into  [ğ‘¤0,ğ‘¤1,ğ‘¤2]âˆˆâ„ğ‘‘Ã—3 . Then, we simply run an LSTM over this sequence of vectors and store the last hidden state outputed by the LSTM: this will be our encoder representation  ğ‘’ . Letâ€™s write the hidden states  [ğ‘’0,ğ‘’1,ğ‘’2]  (and thus  ğ‘’=ğ‘’2 )

## Decoder

Now that we have a vector  ğ‘’  that captures the meaning of the input sequence, weâ€™ll use it to generate the target sequence word by word. Feed to another LSTM cell:  ğ‘’  as hidden state and a special start of sentence vector  ğ‘¤ğ‘ ğ‘œğ‘   as input. The LSTM computes the next hidden state  â„0âˆˆâ„â„ . Then, we apply some function  ğ‘”:â„â„â†¦â„ğ‘‰  so that  ğ‘ 0:=ğ‘”(â„0)âˆˆâ„ğ‘‰  is a vector of the same size as the vocabulary.

â„0ğ‘ 0ğ‘0ğ‘–0=LSTM(ğ‘’,ğ‘¤ğ‘ ğ‘œğ‘ )=ğ‘”(â„0)=softmax(ğ‘ 0)=argmax(ğ‘0)
 
Then, apply a softmax to  ğ‘ 0  to normalize it into a vector of probabilities  ğ‘0âˆˆâ„ğ‘‰  . Now, each entry of  ğ‘0  will measure how likely is each word in the vocabulary. Letâ€™s say that the word â€œcommentâ€ has the highest probability (and thus  ğ‘–0=argmax(ğ‘0)  corresponds to the index of â€œcommentâ€). Get a corresponding vector  ğ‘¤ğ‘–0=ğ‘¤ğ‘ğ‘œğ‘šğ‘šğ‘’ğ‘›ğ‘¡  and repeat the procedure: the LSTM will take  â„0  as hidden state and  ğ‘¤ğ‘ğ‘œğ‘šğ‘šğ‘’ğ‘›ğ‘¡  as input and will output a probability vector  ğ‘1  over the second word, etc.

â„1ğ‘ 1ğ‘1ğ‘–1=LSTM(â„0,ğ‘¤ğ‘–0)=ğ‘”(â„1)=softmax(ğ‘ 1)=argmax(ğ‘1)
 
The decoding stops when the predicted word is a special end of sentence token.
